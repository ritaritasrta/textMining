{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9386f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f11837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f199cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b08c3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"HEY this is Rita ðŸ—£ and welcome to my text mining project. This txt file will be used as an input feeder. I'm just going to type a bunch of stuff here and feed it into the main program that is called text_summarizer. I watched a bunch of YouTube videos and tried to copy different methods and pieced together different variations on how to efficiently write a summarizer program. Feel free to scour the code and download it to run your text file through. It can really help you with summarizing textbooks, books, phamphlets, etc. Enjoy!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d33c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43350c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_main_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readline()\n",
    "        return lines\n",
    "#SUCCESS!!!\n",
    "\n",
    "file_content = open_main_file('textMiningExample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd5ba59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(word):\n",
    "    #get all english stop words from nltk\n",
    "    text_tokens = word.split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    #print(stop_words)\n",
    "\n",
    "    #remove all stop words in english and place relevent text into new file called cleanFile.txt\n",
    "    output = \"\" \n",
    "    for word in text_tokens: \n",
    "        if not (word in stop_words): \n",
    "            output += word + \" \"\n",
    "    return output\n",
    "\n",
    "\n",
    "#print(\" \".join(file_content))\n",
    "#print()\n",
    "#print(remove_stop_words(\" \".join(file_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1722100f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HEY this is Rita ðŸ—£ and welcome to my text mining project This txt file will be used as an input feeder I'm just going to type a bunch of stuff here and feed it into the main program that is called text_summarizer I watched a bunch of YouTube videos and tried to copy different methods and pieced together different variations on how to efficiently write a summarizer program Feel free to scour the code and download it to run your text file through It can really help you with summarizing textbooks, books, phamphlets, etc Enjoy!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(file):\n",
    "    for element in file:\n",
    "        file = file.replace(\". \", \" \")\n",
    "    return file\n",
    "\n",
    "remove_punct(file_content)\n",
    "#print(remove_punct(open_main_file('textMiningExample.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "832b9358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEY this is Rita ðŸ—£ and welcome to my text mining project. ',\n",
       " 'This txt file will be used as an input feeder. ',\n",
       " \"I'm just going to type a bunch of stuff here and feed it into the main program that is called text_summarizer. \",\n",
       " 'I watched a bunch of YouTube videos and tried to copy different methods and pieced together different variations on how to efficiently write a summarizer program. ',\n",
       " 'Feel free to scour the code and download it to run your text file through. ',\n",
       " 'It can really help you with summarizing textbooks, books, phamphlets, etc. ',\n",
       " 'Enjoy! ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_text(file):\n",
    "    wnl = WordNetLemmatizer();\n",
    "    \n",
    "    word_compile_end = []\n",
    "    sentence_content = sent_tokenize(file)\n",
    "    for word in sentence_content:\n",
    "        word = wnl.lemmatize(word) + \" \"\n",
    "        word_compile_end.append(word)\n",
    "    return word_compile_end\n",
    "\n",
    "lemmatize_text(file_content)\n",
    "#unsure if working properly...double check & debug if needed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb2bc750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"hey rita ðŸ—£ welcome text mining project txt file used input feeder i'm going type bunch stuff feed main program called text_summarizer watched bunch youtube videos tried copy different methods pieced together different variations efficiently write summarizer program feel free scour code download run text file really help summarizing textbooks, books, phamphlets, etc enjoy! \"]\n"
     ]
    }
   ],
   "source": [
    "file_content = open_main_file('textMiningExample.txt')\n",
    "file_content = file_content.lower()\n",
    "file_content = remove_punct(file_content)\n",
    "file_content = remove_stop_words(file_content)\n",
    "file_content = lemmatize_text(file_content)\n",
    "\n",
    "print(file_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
